# Detecting of Legitimate URL using Machine Learning

# Abstract:-
Malicious Web sites largely promote the growth of Internet criminal activities and constrain the development of Web services. As a result, there has been strong motivation to develop systemic solutions to stopping the user from visiting such Websites. We propose a learning based approach to classifying Web sites into 3 classes: Benign, Spam and Malicious. Our mechanism only analyzes the Uniform Resource Locator (URL) itself without accessing the content of Web sites. Thus, it eliminates the run-time latency and the possibility of exposing users to the browser based vulnerabilities. By employing learning algorithms, our scheme achieves better performance on generality and coverage compared with blacklisting service.
- URLs of the websites are separated into 3 classes:
  - Benign: Safe websites with normal services
  - Spam: Website performs the act of attempting to flood the user with advertising or sites such as fake surveys and online dating etc
  - Malware: Website created by attackers to disrupt computer operation, gather sensitive information, or gain access to private computer systems.

# Existing System:
A poorly structured NN model may cause the model to underfit the training dataset . On the other hand, exaggeration in restructuring the system to suit every single item in the training dataset may cause the system to be overfitted . One possible solution to avoid the Overfitting problem is by restructuring the NN model in terms of tuning some parameters, adding new neurons to the hidden layer or sometimes adding a new layer to the network. A NN with a small number of hidden neurons may not have a satisfactory representational power to model the complexity and diversity inherent in the data. On the other hand, networks with too many hidden neurons could overfit the data. However, at a certain stage the model can no longer be improved, therefore, the structuring process should be terminated. Hence, an acceptable error rate should be specified when creating any NN model, which itself is considered a problem since it is difficult to determine the acceptable error rate a priori . For instance, the model designer may set the acceptable error rate to a value that is unreachable which causes the model to stick in local minima  or sometimes the model designer may set the acceptable error rate to a value that can further be improved.
  
# Disadvantage:
 1.	It will take time to load all the dataset.
 2.	Process is not accuracy.
 3.	It will analyze slowly.

# System requirements :-
- Hardware requirements:
- Operating system of windows 7, 8, 10(32-bit or 64-bit)
- RAM-4GB
- Software requirements :-
- Anaconda navigator ,jupyter notebook, python language


# MYSELF
- üëã Hi, I‚Äôm @Mithun-1603
- üëÄ I‚Äôm interested in ...HACKING
- üíûÔ∏è I‚Äôm looking to collaborate on ...TECH RELATED TOPICS
- üì´ How to reach me ...PING ME ON EMAIL ...[admin@mithunw3b.tech]
